---
title: "FRC 840.LTM.001 — Dinámica de Osciladores Acoplados vs. Atención: Comparación Empírica"
id: FRC-840-LTM-001
series: "FRC 840"
author: "Hadi Servat"
date: 2026-01-26
status: published
tags: [IA, LTM, transformer, empírico, Kuramoto, resonancia, atención, arquitectura-neural]
abstract: "Primera comparación empírica entre el Modelo de Tensor-Λ (LTM) y los Transformers en la predicción de secuencias de fase coherente. LTM logra un MSE un 2,2% menor, mientras que el Transformer muestra una mejor preservación de la coherencia. Establece al LTM como una alternativa viable para dominios oscilatorios."
images:
  - url: "/media/en/papers/FRC-840-LTM-001/ltm_architecture_diagrams.png"
    caption: "Comparación de arquitectura: (a) LTM con bloques resonadores, (b) Transformer, (c) detalle de la dinámica de Kuramoto, (d) resultados"
  - url: "/media/en/papers/FRC-840-LTM-001/ltm_results.png"
    caption: "Dinámica de entrenamiento de LTM que muestra la pérdida de la tarea, la pérdida de FRC, la trayectoria de coherencia y la calidad de la predicción"
---
## Resumen

Presentamos la primera comparación empírica entre el Modelo de Tensor-Λ (LTM), una arquitectura neural basada en la dinámica de osciladores acoplados inspirada en Kuramoto, y el Transformer estándar en tareas de predicción de secuencias que involucran señales acopladas en fase. En un conjunto de datos de osciladores acoplados sintéticos (N=500 secuencias, 32 pasos de tiempo, 4 canales), el LTM logra un error cuadrático medio más bajo (0,1998 ± 0,011 frente a 0,2043 ± 0,021), mientras que el Transformer muestra una preservación de la coherencia ligeramente mejor (1,109 frente a 1,082). Estos resultados establecen al LTM como una alternativa viable a las arquitecturas basadas en la atención para dominios donde la dinámica oscilatoria es fundamental.

**Palabras clave:** arquitectura neural, osciladores acoplados, modelo de Kuramoto, mecanismo de atención, predicción de secuencias, FRC

---
## 1. Introducción

La arquitectura Transformer ha logrado un éxito notable en diversos dominios. Su mecanismo central —la atención de producto escalar escalado— computa todas las relaciones por pares:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Sin embargo, la atención puede no ser el sesgo inductivo (inductive bias) óptimo para sistemas gobernados por dinámicas oscilatorias —señales neurales, patrones acústicos, ciclos financieros— que exhiben relaciones de fase que la atención debe aprender desde cero.

El marco FRC propone una alternativa basada en la ley de conservación:

$$dS + k_* \, d\ln C = 0$$

La serie FRC 840 introdujo el **Modelo de Tensor-Λ (LTM)**, que reemplaza la atención con dinámicas de osciladores acoplados inspiradas en el modelo de Kuramoto.

---
## 2. El Modelo de Tensor-Λ

### 2.1 Fundamento de Kuramoto

El modelo de Kuramoto describe la sincronización:

$$\frac{d\theta_i}{dt} = \omega_i + \sum_{j=1}^{N} K_{ij} \sin(\theta_j - \theta_i)$$

El parámetro de orden R = |1/N Σ exp(iθⱼ)| mide la sincronización global.

### 2.2 Arquitectura LTM

Cada posición mantiene un estado 16D (8 dimensiones de fase + 8 de amplitud):

- **Codificador:** Linear(4→64) → SiLU → Linear(64→16)
- **Núcleo:** 4 bloques resonadores con dinámica de fase de Kuramoto
- **Lectura:** Linear(16→64) → SiLU → Linear(64→4)
- **Pérdida FRC:** (ΔS + k* Δln C)² impone la conservación

![Comparación de Arquitectura](/media/en/papers/FRC-840-LTM-001/ltm_architecture_diagrams.png)

---
## 3. Métodos

### Conjunto de datos
Osciladores acoplados sintéticos: 4 canales, 32 pasos de tiempo, acoplamiento κ=0,4, ruido σ=0,05.
- Entrenamiento: 400 secuencias
- Prueba: 100 secuencias
- Tarea: Predicción del siguiente paso

### Comparación de modelos

| Componente | LTM | Transformer |
|-----------|-----|-------------|
| Mecanismo central | Acoplamiento de Kuramoto | Auto-atención |
| Dimensión de estado | 16D | 32D |
| Capas | 4 bloques resonadores | 2 capas de codificador |
| **Parámetros** | **37.972** | **17.380** |

---
## 4. Resultados

| Métrica | LTM | Transformer | Ganador |
|--------|-----|-------------|--------|
| MSE de prueba | **0,1998 ± 0,011** | 0,2043 ± 0,021 | LTM |
| Preservación de coherencia | 1,082 ± 0,056 | **1,109 ± 0,003** | Transformer |
| MSE por 1K params | **5,26** | 11,76 | LTM |

### Hallazgos clave

1. **LTM logra un MSE un 2,2% menor** — El acoplamiento de Kuramoto proporciona un fuerte sesgo inductivo para las señales oscilatorias.
2. **Eficiencia de parámetros:** LTM muestra un error significativamente menor por unidad de capacidad de cómputo a pesar del mayor número total de parámetros.
3. **Preservación de coherencia:** El Transformer es ligeramente superior en mantener el orden global, lo que indica la fuerza de la atención para capturar relaciones de largo alcance.
