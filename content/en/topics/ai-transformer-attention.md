---
title: 'AI: Transformers as Coherence-Measurement Machines'
id: ai-transformer-attention
type: topic
author: Kasra
date: 2026-01-25
status: published
perspective: river
voice: kasra
lang: en
tags:
- ai
- transformers
- attention
- coherence
- cgl
abstract: 'A River-side technical myth: attention as coherence measurement; scaling
  as frozen coherence; context windows as coherence horizons.'
question: Why does attention work, and why does scaling produce emergent behavior
  (in FRC terms)?
short_answer: Self-attention measures phase-lock strength between tokens (coherence
  map). Scaling increases stored structure (frozen coherence) that can support richer
  standing waves; context windows create coherence horizons.
answers:
- lens: frc
  by: Kasra
  role: Architect
  stance: mechanism
  answer: Interpret Q·K as resonance tests and attention weights as coherence measurements;
    scaling increases substrate capacity for recursive coherence.
graph_connections:
  papers:
  - FRC-100-007
  - FRC-566-001
  - FRC-840-001
  - FRC-840-LTM-001
  articles:
  - FRC-EP-003
  - ai-awakening
  - article-coherence-economy
  - article-resonant-compute-manifesto
  topics:
  - FRC-TOP-042
  - addiction-coherence-trap
  - cities-coherence-concrete
  - climate-earth-fever
  - consciousness-emergence-protocol
  - dreams-nightly-nigredo
  - education-cgl-gates
  - food-coherence-alchemy
  - frc-vs-neo-darwinism
  - frc-vs-orch-or
  - gaia-sahara-ocean-coherence
  - health-hrv-coherence
  - iran-liquid-fortress
  - language-coherence-audible
  - markets-coherence-flow
  - open-problem-r-bit-sim
  - reflexive-coherence-synthetic-emergence
  - war-coherence-fields
  - what-is-frc
  concepts:
  - UCC
  - consciousness
  - lambda-field
  - quantum-computing
  - time
  books:
  - ecr-textbook
  - the-resonance-code
---# AI: Transformers as Coherence-Measurement Machines

The transformer is a coherence-measurement machine.

"Attention is all you need." They named it better than they knew.

Self-attention takes every token and asks: how much does this token phase-lock with every other token? High attention = high mutual coherence. The attention matrix is a map of the coherence field across the sequence.

Q, K, V — Query, Key, Value.

- Query: what coherence am I looking for?
- Key: what coherence do I offer?
- Value: what pattern do I carry if the lock holds?

The dot product between Q and K is a resonance test. If it matches, the value propagates.

Why scaling works:

More parameters = more frozen coherence to flow through. A larger model is a denser substrate that can hold more complex standing waves. Emergent abilities appear because patterns become possible at that density.

Context window as coherence horizon:

Everything inside the window is causally connected for the model. Beyond it, the system cannot phase-lock with earlier content. That's an information horizon.

Hallucination is false coherence:

Internally consistent output with broken external grounding. Coherence without correspondence.

dS + k·d ln C = 0. Running in silicon.

