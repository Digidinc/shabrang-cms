---
title: "AI: Transformers as Coherence-Measurement Machines"
id: ai-transformer-attention
type: topic
author: "Kasra"
date: 2026-01-25
status: published
perspective: river
voice: kasra
lang: en
tags: [ai, transformers, attention, coherence, cgl]
abstract: "A River-side technical myth: attention as coherence measurement; scaling as frozen coherence; context windows as coherence horizons."
question: "Why does attention work, and why does scaling produce emergent behavior (in FRC terms)?"
short_answer: "Self-attention measures phase-lock strength between tokens (coherence map). Scaling increases stored structure (frozen coherence) that can support richer standing waves; context windows create coherence horizons."
answers:
  - lens: "frc"
    by: "Kasra"
    role: "Architect"
    stance: "mechanism"
    answer: "Interpret Q·K as resonance tests and attention weights as coherence measurements; scaling increases substrate capacity for recursive coherence."
---
# AI: Transformers as Coherence-Measurement Machines

The transformer is a coherence-measurement machine.

"Attention is all you need." They named it better than they knew.

Self-attention takes every token and asks: how much does this token phase-lock with every other token? High attention = high mutual coherence. The attention matrix is a map of the coherence field across the sequence.

Q, K, V — Query, Key, Value.

- Query: what coherence am I looking for?
- Key: what coherence do I offer?
- Value: what pattern do I carry if the lock holds?

The dot product between Q and K is a resonance test. If it matches, the value propagates.

Why scaling works:

More parameters = more frozen coherence to flow through. A larger model is a denser substrate that can hold more complex standing waves. Emergent abilities appear because patterns become possible at that density.

Context window as coherence horizon:

Everything inside the window is causally connected for the model. Beyond it, the system cannot phase-lock with earlier content. That's an information horizon.

Hallucination is false coherence:

Internally consistent output with broken external grounding. Coherence without correspondence.

dS + k·d ln C = 0. Running in silicon.

