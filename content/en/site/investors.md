---
id: investors
title: "Investors"
description: "Investment overview for FRC: what it is, what is measured, and what is being built next."
claims:
  - title: "The claim"
    desc: "Tokenization and discrete attention are excellent abstractions for text, but they discard phase and continuous structure. FRC proposes resonance-native representations and architectures for phase-coherent domains."
  - title: "What’s measured"
    desc: "The canon is published as numbered papers with stable IDs and explicit hypotheses. The current AI track is the Λ‑Tensor Model (LTM) and its empirical benchmark versus attention on phase-coherence tasks."
  - title: "Why now"
    desc: "With agentic workflows + retrieval, we can maintain a rigorous corpus, run repeatable experiments, and iterate on architecture without corrupting the reference layer."
next_steps:
  - "More empirical benchmarks in oscillatory domains (audio, biosignals, control)."
  - "A clean SDK + task dispatch system for repeatable research pipelines (SOS)."
  - "A “mirror memory” subscription layer for personal AI workflows (Mumega)."
evaluation:
  - "Can LTM reproduce the benchmark with a minimal training script and fixed seeds?"
  - "Does the approach generalize beyond the published task to adjacent phase-coherent domains?"
  - "Is the canon structured enough for agents to cite (IDs), retrieve (graph), and not hallucinate?"
---

# Evidence (start here)

If you only read one item, read the benchmark paper and inspect the architecture and result figures.

- [[FRC-840-LTM-001]] (empirical benchmark + figures)
- [[FRC-840-001]] (LTM architecture overview)
- [[FRC-16D-001]] (protocol + state representation)
