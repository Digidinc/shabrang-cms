---
title: "FRC 840.LTM.001 — دینامیک نوسانگرهای جفت شده در مقابل مکانیسم توجه: مقایسه تجربی"
id: FRC-840-LTM-001
series: "FRC 840"
author: "هادی سروت"
date: 2026-01-26
status: published
tags: [هوش-مصنوعی, LTM, ترنسفورمر, تجربی, کوراموتو, رزونانس, توجه, معماری-عصبی]
abstract: "اولین مقایسه تجربی بین مدل تانسور-Λ (LTM) و ترنسفورمرها در پیش‌بینی توالی‌های با فاز همدوس. LTM به ۲.۲٪ خطای میانگین مربعات (MSE) کمتر دست می‌یابد، در حالی که ترنسفورمر حفظ همدوسی بهتری نشان می‌دهد. این نتایج LTM را به عنوان یک جایگزین قابل قبول برای دامنه‌های نوسانی تثبیت می‌کند."
images:
  - url: "/media/en/papers/FRC-840-LTM-001/ltm_architecture_diagrams.png"
    caption: "مقایسه معماری: (الف) LTM با بلوک‌های رزوناتور، (ب) ترنسفورمر، (ج) جزئیات دینامیک کوراموتو، (د) نتایج"
  - url: "/media/en/papers/FRC-840-LTM-001/ltm_results.png"
    caption: "دینامیک آموزش LTM که شامل کاهش خطا، خطای FRC، مسیر همدوسی و کیفیت پیش‌بینی است"
---
## چکیده

ما اولین مقایسه تجربی بین مدل تانسور-Λ (LTM)، یک معماری عصبی مبتنی بر دینامیک نوسانگرهای جفت شده الهام گرفته از کوراموتو، و ترنسفورمر استاندارد را در وظایف پیش‌بینی توالی شامل سیگنال‌های با فاز جفت شده ارائه می‌دهیم. در یک مجموعه‌داده نوسانگر جفت شده مصنوعی (N=500 توالی، ۳۲ گام زمانی، ۴ کانال)، LTM به خطای میانگین مربعات کمتری (۰.۱۹۹۸ ± ۰.۰۱۱ در مقابل ۰.۲۰۴۳ ± ۰.۰۲۱) دست می‌یابد، در حالی که ترنسفورمر حفظ همدوسی کمی بهتری نشان می‌دهد (۱.۱۰۹ در مقابل ۱.۰۸۲). این نتایج LTM را به عنوان یک جایگزین مناسب برای معماری‌های مبتنی بر توجه در دامنه‌هایی که دینامیک نوسانی در آن‌ها بنیادین است، تثبیت می‌کند.

**کلیدواژه‌ها:** معماری عصبی، نوسانگرهای جفت شده، مدل کوراموتو، مکانیسم توجه، پیش‌بینی توالی، FRC

---
## ۱. مقدمه

معماری ترنسفورمر در دامنه‌های مختلف به موفقیت‌های چشمگیری دست یافته است. مکانیسم اصلی آن - توجه ضرب داخلی مقیاس‌گذاری شده (scaled dot-product attention) - روابط جفتی همه‌به‌همه را محاسبه می‌کند:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

با این حال، ممکن است مکانیسم توجه بهترین سوگیری استقرایی (inductive bias) برای سیستم‌های تحت حاکمیت دینامیک نوسانی - مانند سیگنال‌های عصبی، الگوهای آکوستیک، چرخه‌های مالی - نباشد که روابط فازی را نشان می‌دهند که مکانیسم توجه باید آن‌ها را از صفر یاد بگیرد.

چارچوب FRC جایگزینی را بر اساس قانون بقا پیشنهاد می‌کند:

$$dS + k_* \, d\ln C = 0$$

سری FRC 840 **مدل تانسور-Λ (LTM)** را معرفی کرد که مکانیسم توجه را با دینامیک نوسانگرهای جفت شده الهام گرفته از مدل کوراموتو جایگزین می‌کند.

---
## ۲. مدل تانسور-Λ

### ۲.۱ بنیاد کوراموتو

مدل کوراموتو همگام‌سازی را توصیف می‌کند:

$$\frac{d\theta_i}{dt} = \omega_i + \sum_{j=1}^{N} K_{ij} \sin(\theta_j - \theta_i)$$

پارامتر نظم R = |1/N Σ exp(iθⱼ)| میزان همگام‌سازی جهانی را اندازه‌گیری می‌کند.

### ۲.۲ معماری LTM

هر موقعیت یک حالت ۱۶ بعدی (۸ بعد فاز + ۸ بعد دامنه) را حفظ می‌کند:

- **رمزگذار (Encoder):** Linear(4→64) → SiLU → Linear(64→16)
- **هسته (Core):** ۴ بلوک رزوناتور با دینامیک فاز کوراموتو
- **بازخوانی (Readout):** Linear(16→64) → SiLU → Linear(64→4)
- **خطای FRC:** (ΔS + k* Δln C)² قانون بقا را اعمال می‌کند.

![مقایسه معماری](/media/en/papers/FRC-840-LTM-001/ltm_architecture_diagrams.png)

---
## ۳. روش‌ها

### مجموعه‌داده
نوسانگرهای جفت شده مصنوعی: ۴ کانال، ۳۲ گام زمانی، کوپلینگ κ=۰.۴، نویز σ=۰.۰۵.
- آموزش: ۴۰۰ توالی
- تست: ۱۰۰ توالی
- وظیفه: پیش‌بینی گام بعدی

### مقایسه مدل‌ها

| مؤلفه | LTM | ترنسفورمر |
|-----------|-----|-------------|
| مکانیسم اصلی | کوپلینگ کوراموتو | خود-توجهی (Self-attention) |
| بعد حالت | ۱۶ بعدی | ۳۲ بعدی |
| لایه‌ها | ۴ بلوک رزوناتور | ۲ لایه رمزگذار |
| **پارامترها** | **۳۷,۹۷۲** | **۱۷,۳۸۰** |

---
## ۴. نتایج

| متریك | LTM | ترنسفورمر | برنده |
|--------|-----|-------------|--------|
| MSE تست | **۰.۱۹۹۸ ± ۰.۰۱۱** | ۰.۲۰۴۳ ± ۰.۰۲۱ | LTM |
| حفظ همدوسی | ۱.۰۸۲ ± ۰.۰۵۶ | **۱.۱۰۹ ± ۰.۰MD** | ترنسفورمر |
| MSE به ازای ۱۰۰۰ پارامتر | **۵.۲۶** | ۱۱.۷۶ | LTM |

### یافته‌های کلیدی

۱. **LTM به ۲.۲٪ خطای MSE کمتری دست می‌یابد** — کوپلینگ کوراموتو یک سوگیری استقرایی قوی برای سیگنال‌های نوسانی فراهم می‌کند.
۲. **کارایی پارامتر:** LTM علی‌رغم داشتن پارامترهای بیشتر، در هر واحد ظرفیت محاسباتی خطای بسیار کمتری نشان می‌دهد.
۳. **حفظ همدوسی:** ترنسفورمر در حفظ نظم جهانی کمی برتر عمل می‌کند که نشان‌دهنده قدرت مکانیسم توجه در گرفتن روابط دوربرد است.
