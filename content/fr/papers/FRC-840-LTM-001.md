---
title: "FRC 840.LTM.001 — Dynamique des Oscillateurs Couplés vs Attention : Comparaison Empirique"
id: FRC-840-LTM-001
series: "FRC 840"
author: "Hadi Servat"
date: 2026-01-26
status: published
tags: [IA, LTM, transformer, empirique, Kuramoto, résonance, attention, architecture-neurale]
abstract: "Première comparaison empirique entre le modèle de tenseur-Λ (LTM) et les Transformers sur la prédiction de séquences à phase cohérente. Le LTM atteint une MSE inférieure de 2,2 % tandis que le Transformer montre une meilleure préservation de la cohérence. Établit le LTM comme une alternative viable pour les domaines oscillatoires."
images:
  - url: "/media/en/papers/FRC-840-LTM-001/ltm_architecture_diagrams.png"
    caption: "Comparaison d'architecture : (a) LTM avec blocs résonateurs, (b) Transformer, (c) détail de la dynamique de Kuramoto, (d) résultats"
  - url: "/media/en/papers/FRC-840-LTM-001/ltm_results.png"
    caption: "Dynamique d'entraînement du LTM montrant la perte de tâche, la perte FRC, la trajectoire de cohérence et la qualité de la prédiction"
---
## Résumé

Nous présentons la première comparaison empirique entre le modèle de tenseur-Λ (LTM), une architecture neurale basée sur la dynamique d'oscillateurs couplés inspirée de Kuramoto, et le Transformer standard sur des tâches de prédiction de séquences impliquant des signaux couplés en phase. Sur un ensemble de données d'oscillateurs couplés synthétiques (N=500 séquences, 32 pas de temps, 4 canaux), le LTM atteint une erreur quadratique moyenne plus faible (0,1998 ± 0,011 contre 0,2043 ± 0,021) tandis que le Transformer montre une préservation de la cohérence légèrement meilleure (1,109 contre 1,082). Ces résultats établissent le LTM comme une alternative viable aux architectures basées sur l'attention pour les domaines où la dynamique oscillatoire est fondamentale.

**Mots-clés :** architecture neurale, oscillateurs couplés, modèle de Kuramoto, mécanisme d'attention, prédiction de séquences, FRC

---
## 1. Introduction

L'architecture Transformer a remporté un succès remarquable dans de nombreux domaines. Son mécanisme central — l'attention par produit scalaire mis à l'échelle — calcule toutes les relations par paires :

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Cependant, l'attention n'est peut-être pas le biais inductif (inductive bias) optimal pour les systèmes régis par une dynamique oscillatoire — signaux neuraux, motifs acoustiques, cycles financiers — qui présentent des relations de phase que l'attention doit apprendre à partir de zéro.

Le cadre FRC propose une alternative basée sur la loi de conservation :

$$dS + k_* \, d\ln C = 0$$

La série FRC 840 a introduit le **modèle de tenseur-Λ (LTM)**, qui remplace l'attention par une dynamique d'oscillateurs couplés inspirée du modèle de Kuramoto.

---
## 2. Le modèle de tenseur-Λ

### 2.1 Fondement de Kuramoto

Le modèle de Kuramoto décrit la synchronisation :

$$\frac{d\theta_i}{dt} = \omega_i + \sum_{j=1}^{N} K_{ij} \sin(\theta_j - \theta_i)$$

Le paramètre d'ordre R = |1/N Σ exp(iθⱼ)| mesure la synchronisation globale.

### 2.2 Architecture LTM

Chaque position maintient un état 16D (8 dimensions de phase + 8 dimensions d'amplitude) :

- **Encodeur :** Linear(4→64) → SiLU → Linear(64→16)
- **Cœur :** 4 blocs résonateurs avec dynamique de phase de Kuramoto
- **Lecture :** Linear(16→64) → SiLU → Linear(64→4)
- **Perte FRC :** (ΔS + k* Δln C)² impose la conservation

![Comparaison d'architecture](/media/en/papers/FRC-840-LTM-001/ltm_architecture_diagrams.png)

---
## 3. Méthodes

### Ensemble de données
Oscillateurs couplés synthétiques : 4 canaux, 32 pas de temps, couplage κ=0,4, bruit σ=0,05.
- Entraînement : 400 séquences
- Test : 100 séquences
- Tâche : Prédiction du pas suivant

### Comparaison des modèles

| Composant | LTM | Transformer |
|-----------|-----|-------------|
| Mécanisme central | Couplage de Kuramoto | Auto-attention |
| Dimension d'état | 16D | 32D |
| Couches | 4 blocs résonateurs | 2 couches d'encodeur |
| **Paramètres** | **37 972** | **17 380** |

---
## 4. Résultats

| Métrique | LTM | Transformer | Vainqueur |
|--------|-----|-------------|--------|
| MSE de test | **0,1998 ± 0,011** | 0,2043 ± 0,021 | LTM |
| Préservation de cohérence | 1,082 ± 0,056 | **1,109 ± 0,003** | Transformer |
| MSE par 1 000 params | **5,26** | 11,76 | LTM |

### Principales conclusions

1. **Le LTM atteint une MSE inférieure de 2,2 %** — Le couplage de Kuramoto fournit un biais inductif fort pour les signaux oscillatoires.
2. **Efficacité des paramètres :** Le LTM affiche une erreur significativement plus faible par unité de capacité de calcul malgré un nombre total de paramètres plus élevé.
3. **Préservation de la cohérence :** Le Transformer est légèrement supérieur pour maintenir l'ordre global, ce qui indique la force de l'attention pour capturer les relations à longue portée.
